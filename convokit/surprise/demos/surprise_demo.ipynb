{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Surprise With ConvoKit\r\n",
    "=====================\r\n",
    "This notebook provides a demo of how to use the Surprise transformer to compute surprise across a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import numpy as np\n",
    "from convokit import Corpus, download, Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load a corpus\n",
    "--------\n",
    "For now, we will use data from the subreddit r/Cornell to demonstrate the functionality of this transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\rgang\\.convokit\\downloads\\subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('subreddit-Cornell'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 7568\n",
      "Number of Utterances: 74467\n",
      "Number of Conversations: 10744\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the demo, we will take just the top 100 most active speakers (based on the number of conversations they participate in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_BLACKLIST = ['[deleted]', 'DeltaBot', 'AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return utterance.speaker.id not in SPEAKER_BLACKLIST and utterance.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus.organize_speaker_convo_history(utterance_filter=utterance_is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_activities = corpus.get_attribute_table('speaker', ['n_convos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_convos</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>laveritecestla</th>\n      <td>781.0</td>\n    </tr>\n    <tr>\n      <th>EQUASHNZRKUL</th>\n      <td>726.0</td>\n    </tr>\n    <tr>\n      <th>CornHellUniversity</th>\n      <td>696.0</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod</th>\n      <td>647.0</td>\n    </tr>\n    <tr>\n      <th>ilovemymemesboo</th>\n      <td>430.0</td>\n    </tr>\n    <tr>\n      <th>omgdonerkebab</th>\n      <td>425.0</td>\n    </tr>\n    <tr>\n      <th>cartesiancategory</th>\n      <td>341.0</td>\n    </tr>\n    <tr>\n      <th>cornell256</th>\n      <td>330.0</td>\n    </tr>\n    <tr>\n      <th>mushiettake</th>\n      <td>321.0</td>\n    </tr>\n    <tr>\n      <th>Fencerman2</th>\n      <td>298.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    n_convos\nid                          \nlaveritecestla         781.0\nEQUASHNZRKUL           726.0\nCornHellUniversity     696.0\nt3hasiangod            647.0\nilovemymemesboo        430.0\nomgdonerkebab          425.0\ncartesiancategory      341.0\ncornell256             330.0\nmushiettake            321.0\nFencerman2             298.0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_activities.sort_values('n_convos', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers = speaker_activities.sort_values('n_convos', ascending=False).head(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "subset_utts = [list(corpus.get_speaker(speaker).iter_utterances()) for speaker in top_speakers]\n",
    "subset_corpus = Corpus(utterances=list(itertools.chain(*subset_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 100\n",
      "Number of Utterances: 20700\n",
      "Number of Conversations: 6904\n"
     ]
    }
   ],
   "source": [
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create instance of surprise transformer\n",
    "---------------\n",
    "`target_sample_size` and `context_sample_size` specify the minimum number of tokens that should be in the target and context respectively. If the target or context is too short, the transformer will set the surprise to be `nan`. If we sent these to simply be 1, the most surprising statements tend to just be the very short statements. The transformer takes `n_samples` samples from the target and context transformer (where samples are of size corresponding to `target_sample_size` and `context_sample_size`). It calculates cross entropy for each pair of samples and takes the average to get the final surprise score. This is done to minimize effect of length on scores.\n",
    "\n",
    "`model_key_selector` defines how utterances in a corpus should be mapped to a model. It takes in an utterance and returns the key for the corresponding model. For this demo we want to map utterances to models based on their speaker and conversation ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import spacy\r\n",
    "spacy_nlp = spacy.load('en', disable=['ner','parser', 'tagger'])\r\n",
    "surp = Surprise(cv=CountVectorizer(tokenizer=lambda x: [t.text for t in spacy_nlp(x)]), model_key_selector=lambda utt: '_'.join([utt.speaker.id, utt.conversation_id]), target_sample_size=100, context_sample_size=100, n_samples=50, smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Fit transformer to corpus\n",
    "-----\n",
    "The `text_func` parameter defines what text each model should be trained on. For this demo, we want a model corresponding to a (speaker, conversation) pair to be trained on all the utterances from the same speaker in different conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "surp = surp.fit(subset_corpus, text_func=lambda utt: [u.text for u in utt.speaker.iter_utterances() if u.conversation_id != utt.conversation_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Transform corpus\n",
    "--------\n",
    "We'll call `transform` with object type `'speaker'` so that surprise scores will be added as a metadata field for each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [42:59, 25.79s/it]\n"
     ]
    }
   ],
   "source": [
    "transformed_corpus = surp.transform(subset_corpus, 'speaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "------\n",
    "Let's take a look at some of the most surprising speaker conversation involvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "def combine_dicts(x,y):\n",
    "    x.update(y)\n",
    "    return x\n",
    "surprise_scores = reduce(combine_dicts, transformed_corpus.get_speakers_dataframe()['meta.surprise'].values)\n",
    "suprise_series = pd.Series(surprise_scores).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_Straight_Derpin_5kst5l__MODEL_Straight_Derpin_5kst5l              4.588239\nGROUP_Dr_Narwhal_6h08sg__MODEL_Dr_Narwhal_6h08sg                        4.522700\nGROUP_ScottVandeberg_8tlcdl__MODEL_ScottVandeberg_8tlcdl                4.486940\nGROUP_sasha07974_8v40c1__MODEL_sasha07974_8v40c1                        4.479621\nGROUP_mushiettake_89mbvs__MODEL_mushiettake_89mbvs                      4.476114\nGROUP_rrrrrrr1131_8l3xht__MODEL_rrrrrrr1131_8l3xht                      4.473097\nGROUP_chillpillsideeffects_6ns4t8__MODEL_chillpillsideeffects_6ns4t8    4.462805\nGROUP_t3hasiangod_4ufm6z__MODEL_t3hasiangod_4ufm6z                      4.462478\nGROUP_agottler_9iyo8u__MODEL_agottler_9iyo8u                            4.455825\nGROUP_bigred623_6fov8f__MODEL_bigred623_6fov8f                          4.454888\ndtype: float64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_surprising = suprise_series.sort_values(ascending=False).head(10)\n",
    "most_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the least surprising entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_dedicateddan_4krfrc__MODEL_dedicateddan_4krfrc        4.200921\nGROUP_shadowclan98_6njs8z__MODEL_shadowclan98_6njs8z        4.214656\nGROUP_Fencerman2_90r1nf__MODEL_Fencerman2_90r1nf            4.218212\nGROUP_kickstand_obvjl__MODEL_kickstand_obvjl                4.222343\nGROUP_CornellMan333_9j2exy__MODEL_CornellMan333_9j2exy      4.226146\nGROUP_dedicateddan_1zcxhv__MODEL_dedicateddan_1zcxhv        4.226716\nGROUP_cryptkeep_3zgnom__MODEL_cryptkeep_3zgnom              4.227558\nGROUP_Fencerman2_6tiomd__MODEL_Fencerman2_6tiomd            4.227843\nGROUP_CornellMan333_9epekx__MODEL_CornellMan333_9epekx      4.233047\nGROUP_laveritecestla_4pylgl__MODEL_laveritecestla_4pylgl    4.234354\ndtype: float64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_surprising = suprise_series.sort_values(ascending=True).head(10)\n",
    "least_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to SpeakerConvoDiversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import SpeakerConvoDiversity\n",
    "\n",
    "scd = SpeakerConvoDiversity('div', select_fn=lambda df, row, aux: (df.convo_id != row.convo_id) & (df.speaker == row.speaker), speaker_cols=['n_convos'], aux_input={'n_iters': 50, 'cmp_sample_size': 100, 'ref_sample_size': 100}, verbosity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/20700 utterances processed\n",
      "2000/20700 utterances processed\n",
      "3000/20700 utterances processed\n",
      "4000/20700 utterances processed\n",
      "5000/20700 utterances processed\n",
      "6000/20700 utterances processed\n",
      "7000/20700 utterances processed\n",
      "8000/20700 utterances processed\n",
      "9000/20700 utterances processed\n",
      "10000/20700 utterances processed\n",
      "11000/20700 utterances processed\n",
      "12000/20700 utterances processed\n",
      "13000/20700 utterances processed\n",
      "14000/20700 utterances processed\n",
      "15000/20700 utterances processed\n",
      "16000/20700 utterances processed\n",
      "17000/20700 utterances processed\n",
      "18000/20700 utterances processed\n",
      "19000/20700 utterances processed\n",
      "20000/20700 utterances processed\n",
      "20700/20700 utterances processed\n"
     ]
    }
   ],
   "source": [
    "from convokit.text_processing import TextParser\n",
    "\n",
    "tokenizer = TextParser(mode='tokenize', output_field='tokens', verbosity=1000)\n",
    "subset_corpus = tokenizer.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining tokens across conversation utterances\n",
      "100 / 15394\n",
      "200 / 15394\n",
      "300 / 15394\n",
      "400 / 15394\n",
      "500 / 15394\n",
      "600 / 15394\n",
      "700 / 15394\n",
      "800 / 15394\n",
      "900 / 15394\n",
      "1000 / 15394\n",
      "1100 / 15394\n",
      "1200 / 15394\n",
      "1300 / 15394\n",
      "1400 / 15394\n",
      "1500 / 15394\n",
      "1600 / 15394\n",
      "1700 / 15394\n",
      "1800 / 15394\n",
      "1900 / 15394\n",
      "2000 / 15394\n",
      "2100 / 15394\n",
      "2200 / 15394\n",
      "2300 / 15394\n",
      "2400 / 15394\n",
      "2500 / 15394\n",
      "2600 / 15394\n",
      "2700 / 15394\n",
      "2800 / 15394\n",
      "2900 / 15394\n",
      "3000 / 15394\n",
      "3100 / 15394\n",
      "3200 / 15394\n",
      "3300 / 15394\n",
      "3400 / 15394\n",
      "3500 / 15394\n",
      "3600 / 15394\n",
      "3700 / 15394\n",
      "3800 / 15394\n",
      "3900 / 15394\n",
      "4000 / 15394\n",
      "4100 / 15394\n",
      "4200 / 15394\n",
      "4300 / 15394\n",
      "4400 / 15394\n",
      "4500 / 15394\n",
      "4600 / 15394\n",
      "4700 / 15394\n",
      "4800 / 15394\n",
      "4900 / 15394\n",
      "5000 / 15394\n",
      "5100 / 15394\n",
      "5200 / 15394\n",
      "5300 / 15394\n",
      "5400 / 15394\n",
      "5500 / 15394\n",
      "5600 / 15394\n",
      "5700 / 15394\n",
      "5800 / 15394\n",
      "5900 / 15394\n",
      "6000 / 15394\n",
      "6100 / 15394\n",
      "6200 / 15394\n",
      "6300 / 15394\n",
      "6400 / 15394\n",
      "6500 / 15394\n",
      "6600 / 15394\n",
      "6700 / 15394\n",
      "6800 / 15394\n",
      "6900 / 15394\n",
      "7000 / 15394\n",
      "7100 / 15394\n",
      "7200 / 15394\n",
      "7300 / 15394\n",
      "7400 / 15394\n",
      "7500 / 15394\n",
      "7600 / 15394\n",
      "7700 / 15394\n",
      "7800 / 15394\n",
      "7900 / 15394\n",
      "8000 / 15394\n",
      "8100 / 15394\n",
      "8200 / 15394\n",
      "8300 / 15394\n",
      "8400 / 15394\n",
      "8500 / 15394\n",
      "8600 / 15394\n",
      "8700 / 15394\n",
      "8800 / 15394\n",
      "8900 / 15394\n",
      "9000 / 15394\n",
      "9100 / 15394\n",
      "9200 / 15394\n",
      "9300 / 15394\n",
      "9400 / 15394\n",
      "9500 / 15394\n",
      "9600 / 15394\n",
      "9700 / 15394\n",
      "9800 / 15394\n",
      "9900 / 15394\n",
      "10000 / 15394\n",
      "10100 / 15394\n",
      "10200 / 15394\n",
      "10300 / 15394\n",
      "10400 / 15394\n",
      "10500 / 15394\n",
      "10600 / 15394\n",
      "10700 / 15394\n",
      "10800 / 15394\n",
      "10900 / 15394\n",
      "11000 / 15394\n",
      "11100 / 15394\n",
      "11200 / 15394\n",
      "11300 / 15394\n",
      "11400 / 15394\n",
      "11500 / 15394\n",
      "11600 / 15394\n",
      "11700 / 15394\n",
      "11800 / 15394\n",
      "11900 / 15394\n",
      "12000 / 15394\n",
      "12100 / 15394\n",
      "12200 / 15394\n",
      "12300 / 15394\n",
      "12400 / 15394\n",
      "12500 / 15394\n",
      "12600 / 15394\n",
      "12700 / 15394\n",
      "12800 / 15394\n",
      "12900 / 15394\n",
      "13000 / 15394\n",
      "13100 / 15394\n",
      "13200 / 15394\n",
      "13300 / 15394\n",
      "13400 / 15394\n",
      "13500 / 15394\n",
      "13600 / 15394\n",
      "13700 / 15394\n",
      "13800 / 15394\n",
      "13900 / 15394\n",
      "14000 / 15394\n",
      "14100 / 15394\n",
      "14200 / 15394\n",
      "14300 / 15394\n",
      "14400 / 15394\n",
      "14500 / 15394\n",
      "14600 / 15394\n",
      "14700 / 15394\n",
      "14800 / 15394\n",
      "14900 / 15394\n",
      "15000 / 15394\n",
      "15100 / 15394\n",
      "15200 / 15394\n",
      "15300 / 15394\n"
     ]
    }
   ],
   "source": [
    "div_transformed = scd.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the speaker convo entries that have the highest diversity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>convo_id</th>\n      <th>convo_idx</th>\n      <th>div</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Straight_Derpin__5kst5l</th>\n      <td>Straight_Derpin</td>\n      <td>5kst5l</td>\n      <td>34</td>\n      <td>4.588329</td>\n    </tr>\n    <tr>\n      <th>Dr_Narwhal__6h08sg</th>\n      <td>Dr_Narwhal</td>\n      <td>6h08sg</td>\n      <td>75</td>\n      <td>4.531305</td>\n    </tr>\n    <tr>\n      <th>ScottVandeberg__8tlcdl</th>\n      <td>ScottVandeberg</td>\n      <td>8tlcdl</td>\n      <td>81</td>\n      <td>4.492559</td>\n    </tr>\n    <tr>\n      <th>sasha07974__8v40c1</th>\n      <td>sasha07974</td>\n      <td>8v40c1</td>\n      <td>42</td>\n      <td>4.491322</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__4ufm6z</th>\n      <td>t3hasiangod</td>\n      <td>4ufm6z</td>\n      <td>262</td>\n      <td>4.488475</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__5v6sqb</th>\n      <td>t3hasiangod</td>\n      <td>5v6sqb</td>\n      <td>590</td>\n      <td>4.487068</td>\n    </tr>\n    <tr>\n      <th>rrrrrrr1131__8l3xht</th>\n      <td>rrrrrrr1131</td>\n      <td>8l3xht</td>\n      <td>25</td>\n      <td>4.486768</td>\n    </tr>\n    <tr>\n      <th>SwissWatchesOnly__9hcpip</th>\n      <td>SwissWatchesOnly</td>\n      <td>9hcpip</td>\n      <td>129</td>\n      <td>4.485485</td>\n    </tr>\n    <tr>\n      <th>agottler__9iyo8u</th>\n      <td>agottler</td>\n      <td>9iyo8u</td>\n      <td>66</td>\n      <td>4.476691</td>\n    </tr>\n    <tr>\n      <th>cartesiancategory__8bdf5g</th>\n      <td>cartesiancategory</td>\n      <td>8bdf5g</td>\n      <td>310</td>\n      <td>4.475636</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                     speaker convo_id  convo_idx       div\nid                                                                        \nStraight_Derpin__5kst5l      Straight_Derpin   5kst5l         34  4.588329\nDr_Narwhal__6h08sg                Dr_Narwhal   6h08sg         75  4.531305\nScottVandeberg__8tlcdl        ScottVandeberg   8tlcdl         81  4.492559\nsasha07974__8v40c1                sasha07974   8v40c1         42  4.491322\nt3hasiangod__4ufm6z              t3hasiangod   4ufm6z        262  4.488475\nt3hasiangod__5v6sqb              t3hasiangod   5v6sqb        590  4.487068\nrrrrrrr1131__8l3xht              rrrrrrr1131   8l3xht         25  4.486768\nSwissWatchesOnly__9hcpip    SwissWatchesOnly   9hcpip        129  4.485485\nagottler__9iyo8u                    agottler   9iyo8u         66  4.476691\ncartesiancategory__8bdf5g  cartesiancategory   8bdf5g        310  4.475636"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the diversity scores returned by `SpeakerConvoDiversity` are slightly different from the scores returned by the `Surprise` transformer. This difference can be attributed to the addition of Laplace smoothing in the `Surprise` transformer to account for out of vocabulary tokens. The `SpeakerConvoDiversity` transformer deals with OOV tokens by simply treating their count as 1. If you run the `Surprise` transformer with the `smooth` flag set to false, the transformer will treat OOV tokens the same way `SpeakerConvoDiversity` does. When run without smoothing, the `Surprise` transformer returns the same scores as `SpeakerConvoDiversity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the least diverse speaker-convo entries based on the SpeakerConvoDiversity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>convo_id</th>\n      <th>convo_idx</th>\n      <th>div</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fencerman2__90r1nf</th>\n      <td>Fencerman2</td>\n      <td>90r1nf</td>\n      <td>231</td>\n      <td>4.212314</td>\n    </tr>\n    <tr>\n      <th>happysted__7qeh50</th>\n      <td>happysted</td>\n      <td>7qeh50</td>\n      <td>23</td>\n      <td>4.214947</td>\n    </tr>\n    <tr>\n      <th>shadowclan98__9mfj81</th>\n      <td>shadowclan98</td>\n      <td>9mfj81</td>\n      <td>97</td>\n      <td>4.221873</td>\n    </tr>\n    <tr>\n      <th>cornell256__96utv3</th>\n      <td>cornell256</td>\n      <td>96utv3</td>\n      <td>292</td>\n      <td>4.223942</td>\n    </tr>\n    <tr>\n      <th>Fencerman2__6tiomd</th>\n      <td>Fencerman2</td>\n      <td>6tiomd</td>\n      <td>111</td>\n      <td>4.227082</td>\n    </tr>\n    <tr>\n      <th>kickstand__obvjl</th>\n      <td>kickstand</td>\n      <td>obvjl</td>\n      <td>6</td>\n      <td>4.228307</td>\n    </tr>\n    <tr>\n      <th>laveritecestla__4pylgl</th>\n      <td>laveritecestla</td>\n      <td>4pylgl</td>\n      <td>74</td>\n      <td>4.230566</td>\n    </tr>\n    <tr>\n      <th>dedicateddan__4krfrc</th>\n      <td>dedicateddan</td>\n      <td>4krfrc</td>\n      <td>97</td>\n      <td>4.234085</td>\n    </tr>\n    <tr>\n      <th>dedicateddan__1zcxhv</th>\n      <td>dedicateddan</td>\n      <td>1zcxhv</td>\n      <td>20</td>\n      <td>4.234104</td>\n    </tr>\n    <tr>\n      <th>cornell256__4g9uub</th>\n      <td>cornell256</td>\n      <td>4g9uub</td>\n      <td>34</td>\n      <td>4.236136</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                               speaker convo_id  convo_idx       div\nid                                                                  \nFencerman2__90r1nf          Fencerman2   90r1nf        231  4.212314\nhappysted__7qeh50            happysted   7qeh50         23  4.214947\nshadowclan98__9mfj81      shadowclan98   9mfj81         97  4.221873\ncornell256__96utv3          cornell256   96utv3        292  4.223942\nFencerman2__6tiomd          Fencerman2   6tiomd        111  4.227082\nkickstand__obvjl             kickstand    obvjl          6  4.228307\nlaveritecestla__4pylgl  laveritecestla   4pylgl         74  4.230566\ndedicateddan__4krfrc      dedicateddan   4krfrc         97  4.234085\ndedicateddan__1zcxhv      dedicateddan   1zcxhv         20  4.234104\ncornell256__4g9uub          cornell256   4g9uub         34  4.236136"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convokit-venv",
   "language": "python",
   "name": "convokit-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}